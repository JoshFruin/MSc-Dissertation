# -*- coding: utf-8 -*-
"""DenseNet169_CBIS-DDSM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/densenet169-cbis-ddsm-e3b0ee1b-27c2-4ccf-9bb7-7bdbbf1bdc99.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240520/auto/storage/goog4_request%26X-Goog-Date%3D20240520T071149Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0104e4d6e49bb65de0535e9e721aa1d3b28fccdaf455d75341a0264eb1e6d36d7e5e12fb0b5af6b16d3c1ae9158136ddc15d3598608c97f814e666a384adee450bcd834994c30f8a79db77bef78e098321823a3c6642b9a72ef5e07f0ac225d04125389015dfb359470c6e933c34571ec343ec5dfdc04c34278c82087b38538cecb435a2217268a859fd81c74944b5f8cb672475b2e912ae0b9fefbbbd3747c8d35310686825e47f0a599359c461ad9660d232e650cac61c7fed0222f0ccebc57f1c352976b9810196de6530c550c5e21b6572674594b5e9b3f5e5bc0b428cf87ed3bd3100f5d64918ba0a5ed832c88e84047653d41b559af94f7c06994c0fea
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'cbis-ddsm-breast-cancer-image-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1115384%2F1873742%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240520%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240520T071149Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0211063672f94e3bb2aa7d5bcc14790dbb1ac6255a09289ce4d334a8bec28cd4168475727b2e2c31a7efbdb2e61ba53d0b3b53bbadb8f720f7f73dea5d1604fe7efdd8b8d9ac8b8432b7c93f0e27dc622efdbf18ca56a033ebcb3d1914e6e5681cbb5a9b4a647af9b6cfcd2810fc257fb9500a4a5bcbe096136e8f514334cc927c0bd69288208c59770c653c6165ff56b54a7110431b5e95d08f5bdb85a6f568993375027fd9c242838deb7000ca85904e456892b5b8ed833ad8b4a961d6d506059ec391bf0386a0f2703687caee5443ea80207bf5d06df9d328c50d6ba6d4aba7f7ec775ae187fa91a50ddae8cea06398c54e953e90281fad3aeb46bce703d4,densenet/keras/densenet121_imagenet/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F4590%2F5817%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240520%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240520T071149Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D027735704f7fdaf316a220543b1e416039f4855444ed501d4105fac60c1bb3e2126f404e2558bef9beeead44f7e81c3af9f6c622c2a2fbb23796a82caa4159b11eb0b982fde0072c25f2bf5aa729565401d5661068f8404cfbcb2f60d0684b2e579397be985953626c06ae63e9fd99fc54311dd496b9f6c94ed6f1a91550d8efcc38154aadecccfac3a8cdb50e558e816f1bc783d27aa08f69e2c3fd27719bc56b5fa5a98b1912d87f7984bb65c6b9536cd14d6ce0137e740dc9ee600086bbc2fa9a23ecec7210cbe66c059d5ad833f88ab9dca868f8d76845bd2486e6e1557eead743a85604b0a8a5aaa800112a8a61b6892854484aecda48c32d83878827e0'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""### Breast Cancer Detection using DenseNet 169 | CBIS-DDSM

##### I. Import Libraries, Loading Data, Define, etc.
"""

import sys
import os
from os import listdir
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
import pandas as pd
import PIL
import random
random.seed(100)
np.random.seed(100)

# Display some images
import matplotlib.image as mpimg

import tensorflow as tf

import cv2

from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
# Import necessary TensorFlow libraries

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import plot_model

from tensorflow.keras.applications.resnet50 import ResNet50

from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras.models import Model

from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.applications.densenet import DenseNet169
from tensorflow.keras.callbacks import EarlyStopping
from keras.utils import to_categorical

import warnings

# Suppress all warnings globally
warnings.filterwarnings("ignore")

# get the current working directory
current_working_directory = os.getcwd()

# print output to the console
print(current_working_directory)

# output will look something similar to this on a macOS system
# /Users/dionysialemonaki/Documents/my-projects/python-project

# Provide the correct path to the CSV file
csv_path = '/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/meta.csv'

# Read the CSV file into a DataFrame
df_meta = pd.read_csv(csv_path)

# Display the DataFrame
df_meta

dicom_data = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/dicom_info.csv')
dicom_data.head()

dicom_data.info()

dicom_data.SeriesDescription.unique()

image_dir = '/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/jpeg'
full_mammogram_images = dicom_data[dicom_data.SeriesDescription == 'full mammogram images'].image_path
cropped_images = dicom_data[dicom_data.SeriesDescription == 'cropped images'].image_path
roi_mask_images = dicom_data[dicom_data.SeriesDescription == 'ROI mask images'].image_path

full_mammogram_images = full_mammogram_images.apply(lambda x: x.replace('CBIS-DDSM/jpeg', image_dir))
cropped_images = cropped_images.apply(lambda x: x.replace('CBIS-DDSM/jpeg', image_dir))
roi_mask_images = roi_mask_images.apply(lambda x: x.replace('CBIS-DDSM/jpeg', image_dir))
full_mammogram_images.iloc[0]

full_mammogram_images.shape

cropped_images.iloc[0]

cropped_images.shape

roi_mask_images.iloc[0]

roi_mask_images.shape

full_mammogram_dict = dict()
cropped_dict = dict()
roi_mask_dict = dict()

for dicom in full_mammogram_images:
    # print(dicom)
    key = dicom.split("/")[5]
    # print(key)
    full_mammogram_dict[key] = dicom
for dicom in cropped_images:
    key = dicom.split("/")[5]
    cropped_dict[key] = dicom
for dicom in roi_mask_images:
    key = dicom.split("/")[5]
    roi_mask_dict[key] = dicom

next(iter((full_mammogram_dict.items())))

sys.getsizeof(full_mammogram_dict)

next(iter((cropped_dict.items())))

sys.getsizeof(cropped_images)

next(iter((roi_mask_dict.items())))

sys.getsizeof(roi_mask_images)

"""mass and calci csv read (train and test)"""

mass_train_data = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_train_set.csv')
mass_test_data = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/mass_case_description_test_set.csv')
calc_train_data = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_train_set.csv')
calc_test_data = pd.read_csv('/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/csv/calc_case_description_test_set.csv')

mass_train_data.head()

# mass_data = mass_train_data.append(mass_test_data)
mass_test_data.head()

calc_train_data.head()

calc_test_data.head()

# Count the number of benign and malignant cases in the training set
train_counts = mass_train_data['pathology'].value_counts()

# Count the number of benign and malignant cases in the test set
test_counts = mass_test_data['pathology'].value_counts()

# Display the results
print("Mass Training Set:")
print(train_counts)

print("\nMass Test Set:")
print(test_counts)

# Count the number of benign and malignant cases in the training set
train_counts = calc_train_data['pathology'].value_counts()

# Count the number of benign and malignant cases in the test set
test_counts = calc_test_data['pathology'].value_counts()

# Display the results
print("Calc Training Set:")
print(train_counts)

print("\nCalc Test Set:")
print(test_counts)

"""Path fix for mass"""

def fix_image_path_mass(dataset):
    for i, img in enumerate(dataset.values):
        img_name = img[11].split("/")[2]
        if img_name in full_mammogram_dict:
            dataset.iloc[i, 11] = full_mammogram_dict[img_name]

        img_name = img[12].split("/")[2]
        if img_name in cropped_dict:
            dataset.iloc[i, 12] = cropped_dict[img_name]

        img_name = img[13].split("/")[2]
        if img_name in roi_mask_dict:
            dataset.iloc[i, 13] = roi_mask_dict[img_name]

fix_image_path_mass(mass_train_data)

fix_image_path_mass(mass_test_data)

mass_train_data

mass_test_data

"""Path fix for Calc"""

def fix_image_path_calc(dataset):
    for i, img in enumerate(dataset.values):
        img_name = img[11].split("/")[2]
        if img_name in full_mammogram_dict:
            dataset.iloc[i, 11] = full_mammogram_dict[img_name]

        img_name = img[12].split("/")[2]
        if img_name in cropped_dict:
            dataset.iloc[i, 12] = cropped_dict[img_name]

        img_name = img[13].split("/")[2]
        if img_name in roi_mask_dict:
            dataset.iloc[i, 13] = roi_mask_dict[img_name]

fix_image_path_calc(calc_train_data)

calc_train_data

fix_image_path_mass(calc_test_data)

calc_test_data

"""##### II. Data Cleaning"""

# check unique values in pathology column
mass_train_data.pathology.unique()

calc_train_data.pathology.unique()

mass_train_data.info()

calc_train_data.info()

# rename columns
mass_train = mass_train_data.rename(columns={'left or right breast': 'left_or_right_breast',
                                           'image view': 'image_view',
                                           'abnormality id': 'abnormality_id',
                                           'abnormality type': 'abnormality_type',
                                           'mass shape': 'mass_shape',
                                           'mass margins': 'mass_margins',
                                           'image file path': 'image_file_path',
                                           'cropped image file path': 'cropped_image_file_path',
                                           'ROI mask file path': 'ROI_mask_file_path'})

mass_train.head()

# rename columns
calc_train = calc_train_data.rename(columns={'left or right breast': 'left_or_right_breast',
                                             'breast density':'breast_density',
                                           'image view': 'image_view',
                                           'abnormality id': 'abnormality_id',
                                           'abnormality type': 'abnormality_type',
                                           'calc type': 'calc_type',
                                           'calc distribution': 'calc_distribution',
                                           'image file path': 'image_file_path',
                                           'cropped image file path': 'cropped_image_file_path',
                                           'ROI mask file path': 'ROI_mask_file_path'})

calc_train.head()

# check for null values
mass_train.isnull().sum()

calc_train.isnull().sum()

# fill in missing values using the backwards fill method
mass_train['mass_shape'] = mass_train['mass_shape'].fillna(method='bfill')
mass_train['mass_margins'] = mass_train['mass_margins'].fillna(method='bfill')

#check null values
mass_train.isnull().sum()

# fill in missing values using the backwards fill method
calc_train['calc_type'] = calc_train['calc_type'].fillna(method='bfill')
calc_train['calc_distribution'] = calc_train['calc_distribution'].fillna(method='bfill')

#check null values
calc_train.isnull().sum()

mass_test_data.isnull().sum()

calc_test_data.isnull().sum()

# check for column names in mass_test
print(mass_test_data.columns)
print('\n')
# rename columns
mass_test = mass_test_data.rename(columns={'left or right breast': 'left_or_right_breast',
                                           'image view': 'image_view',
                                           'abnormality id': 'abnormality_id',
                                           'abnormality type': 'abnormality_type',
                                           'mass shape': 'mass_shape',
                                           'mass margins': 'mass_margins',
                                           'image file path': 'image_file_path',
                                           'cropped image file path': 'cropped_image_file_path',
                                           'ROI mask file path': 'ROI_mask_file_path'})

# view renamed columns
mass_test.columns

# check for column names in mass_test
print(calc_test_data.columns)
print('\n')
# rename columns
calc_test = calc_test_data.rename(columns={'left or right breast': 'left_or_right_breast',
                                           'breast density':'breast_density',
                                           'image view': 'image_view',
                                           'abnormality id': 'abnormality_id',
                                           'abnormality type': 'abnormality_type',
                                           'calc type': 'calc_type',
                                           'calc distribution': 'calc_distribution',
                                           'image file path': 'image_file_path',
                                           'cropped image file path': 'cropped_image_file_path',
                                           'ROI mask file path': 'ROI_mask_file_path'})

# view renamed columns
calc_test.columns

# fill in missing values using the backwards fill method
calc_test['calc_type'] = calc_test['calc_type'].fillna(method='bfill')
calc_test['calc_distribution'] = calc_test['calc_distribution'].fillna(method='bfill')
#check null values
calc_test.isnull().sum()

"""##### III. Data Visualization"""

# quantitative summary of features
mass_train.describe()

calc_train.describe()

# check datasets shape
print(f'Shape of mass_train: {mass_train.shape}')
print(f'Shape of mass_test: {mass_test.shape}')

# check datasets shape
print(f'Shape of calc_train: {calc_train.shape}')
print(f'Shape of calc_test: {calc_test.shape}')

# pathology distributions
value = mass_train['pathology'].value_counts() + calc_train['pathology'].value_counts()
plt.figure(figsize=(8,6))

plt.pie(value, labels=value.index, autopct='%1.1f%%')
plt.title('Breast Cancer Mass Types', fontsize=12)
plt.show()

# Assuming mass_train and calc_train are your DataFrames

# Set the color palette for mass_train
mass_palette = sns.color_palette("viridis", n_colors=len(mass_train['assessment'].unique()))
sns.countplot(data=mass_train, y='assessment', hue='pathology', palette=mass_palette)
plt.title('Count Plot for mass_train')
plt.show()

# Set the color palette for calc_train
calc_palette = sns.color_palette("magma", n_colors=len(calc_train['assessment'].unique()))
sns.countplot(data=calc_train, y='assessment', hue='pathology', palette=calc_palette)
plt.title('Count Plot for calc_train')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(data=mass_train, x='subtlety', palette='viridis', hue='subtlety')
plt.title('Breast Cancer Mass Subtlety', fontsize=12)
plt.xlabel('Subtlety Grade')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(data=calc_train, x='subtlety', palette='magma', hue='subtlety')
plt.title('Breast Cancer Calc Subtlety', fontsize=12)
plt.xlabel('Subtlety Grade')
plt.ylabel('Count')
plt.show()

# view breast mass shape distribution against pathology
plt.figure(figsize=(8,6))

sns.countplot(mass_train, x='mass_shape', hue='pathology')
plt.title('Mass Shape Distribution by Pathology', fontsize=14)
plt.xlabel('Mass Shape')
plt.xticks(rotation=30, ha='right')
plt.ylabel('Pathology Count')
plt.legend()
plt.show()

plt.figure(figsize=(12, 8))

sns.countplot(data=calc_train, y='calc_type', hue='pathology', palette='viridis')
plt.title('Calcification Type Distribution by Pathology', fontsize=14)
plt.xlabel('Pathology Count')
plt.ylabel('Calc Type')

# Adjust the rotation of the y-axis labels
plt.yticks(rotation=0, ha='right')

# Move the legend outside the plot for better visibility
plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1))

plt.show()

# breast density against pathology
plt.figure(figsize=(8,6))

sns.countplot(mass_train, x='breast_density', hue='pathology')
plt.title('Breast Density vs Pathology\n\n1: fatty || 2: Scattered Fibroglandular Density\n3: Heterogenously Dense || 4: Extremely Dense',
          fontsize=14)
plt.xlabel('Density Grades')
plt.ylabel('Count')
plt.legend()

plt.show()

# breast density against pathology
plt.figure(figsize=(8,6))

sns.countplot(calc_train, x='breast_density', hue='pathology')
plt.title('Breast Density vs Pathology\n\n1: fatty || 2: Scattered Fibroglandular Density\n3: Heterogenously Dense || 4: Extremely Dense',
          fontsize=14)
plt.xlabel('Density Grades')
plt.ylabel('Count')
plt.legend()

plt.show()

mass_train.head()

calc_train.head()

import matplotlib.image as mpimg

def display_images(column, number):
    """displays images in the dataset"""
    # create figure and axes
    number_to_visualize = number
    rows = 1
    cols = number_to_visualize
    fig, axes = plt.subplots(rows, cols, figsize=(15, 5))

    # Loop through rows and display images
    for index, row in mass_train.head(number_to_visualize).iterrows():
        image_path = row[column]
        print(image_path)
        # Check if the file exists
        if os.path.exists(image_path):
            image = mpimg.imread(image_path)
            ax = axes[index]
            ax.imshow(image, cmap='gray')
            ax.set_title(f"{row['pathology']}")
            ax.axis('off')
        else:
            print(f"File not found: {image_path}")

    plt.tight_layout()
    plt.show()

print('Mass Training Dataset\n\n')
print('Full Mammograms:\n')
display_images('image_file_path', 5)
print('Cropped Mammograms:\n')
display_images('cropped_image_file_path', 5)
print('ROI Images:\n')
display_images('ROI_mask_file_path', 5)

def display_images(column, number):
    """displays images in the dataset"""
    # create figure and axes
    number_to_visualize = number
    rows = 1
    cols = number_to_visualize
    fig, axes = plt.subplots(rows, cols, figsize=(15, 5))

    # Loop through rows and display images
    for index, row in calc_train.head(number_to_visualize).iterrows():
        image_path = row[column]
        print(image_path)
        # Check if the file exists
        if os.path.exists(image_path):
            image = mpimg.imread(image_path)
            ax = axes[index]
            ax.imshow(image, cmap='gray')
            ax.set_title(f"{row['pathology']}")
            ax.axis('off')
        else:
            print(f"File not found: {image_path}")

    plt.tight_layout()
    plt.show()


print('Calcification Trianing Dataset\n\n')
print('Full Mammograms:\n')
display_images('image_file_path', 5)
print('Cropped Mammograms:\n')
display_images('cropped_image_file_path', 5)
print('ROI Images:\n')
display_images('ROI_mask_file_path', 5)

# Merge datasets
mass_calc = pd.concat([mass_train, mass_test, calc_train, calc_test], axis=0)

# Define the target size
target_size = (224, 224, 3)

mass_calc

"""##### IV. Image Preprocessing

def apply_gaussian_filter(image, sigma=1.0):
    Apply Gaussian filter to the image
    return cv2.GaussianBlur(image, (0, 0), sigma)

def apply_clahe(image, clip_limit=2.0, tile_grid_size=(8, 8)):
    Apply CLAHE to enhance image contrast.
    lab = cv2.cvtColor(image, cv2.COLOR_RGB2LAB)
    lab_planes = list(cv2.split(lab))  # Convert to list

    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
    lab_planes[0] = clahe.apply(lab_planes[0])

    lab = cv2.merge(lab_planes)
    return cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
"""

# def image_processor(image_path, target_size, base_directory="/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/jpeg"):
#    """Preprocess images for Inception V3 model with Gaussian filter and CLAHE"""
#    absolute_image_path = os.path.abspath(image_path)

    # Counter for skipped and processed images
#    skipped_count = 0
#    processed_count = 0

    # Check if the image path starts with the specified directory
#    if not absolute_image_path.startswith(base_directory):
#        skipped_count += 1
#        return None, skipped_count, processed_count

#    image = cv2.imread(absolute_image_path)
#    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Apply Gaussian filter
    # image = apply_gaussian_filter(image, sigma=1.0)

    # Resize the image
#    image = cv2.resize(image, (target_size[1], target_size[0]))

    # Apply CLAHE
    # image = apply_clahe(image, clip_limit=2.0, tile_grid_size=(8, 8))

    # Normalize pixel values to be in the range [0, 1]
#    image_array = image / 255.0

#    processed_count += 1

#    return image_array, skipped_count, processed_count

def image_processor(image_path, target_size, base_directory="/kaggle/input/cbis-ddsm-breast-cancer-image-dataset/jpeg"):
    """Preprocess images for Inception V3 model with Gaussian filter and CLAHE"""

    # Check if a GPU is available
    physical_devices = tf.config.list_physical_devices('GPU')
    if len(physical_devices) == 0:
        print("Warning: No GPU devices found. Using CPU.")
    else:
        # print("GPU found. Using GPU for image processing.")
        tf.config.experimental.set_memory_growth(physical_devices[0], True)

    absolute_image_path = os.path.abspath(image_path)

    # Counter for skipped and processed images
    skipped_count = 0
    processed_count = 0

    # Check if the image path starts with the specified directory
    if not absolute_image_path.startswith(base_directory):
        skipped_count += 1
        return None, skipped_count, processed_count

    # Read the image using TensorFlow for GPU acceleration
    image = tf.io.read_file(absolute_image_path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)

    # Resize the image
    image = tf.image.resize(image, (target_size[0], target_size[1]))

    # Normalize pixel values to be in the range [0, 1]
    image_array = image.numpy()

    processed_count += 1

    return image_array, skipped_count, processed_count

# Example usage with target_size for both 'image_file_path' and 'cropped_image_file_path'
target_size = (224, 224, 3)  # Set your desired target size

# Apply preprocessor to train data
result = mass_calc['image_file_path'].apply(lambda x: pd.Series(image_processor(x, target_size), index=['image_array', 'skipped_count', 'processed_count']))

# Extract processed images, skipped count, and processed count from the result
mass_calc[['processed_images', 'skipped_count', 'processed_count']] = result

# Filter out rows where processed_images is None
mass_calc = mass_calc.dropna(subset=['processed_images'])

# Convert the processed_images column to an array
X_resized = np.array(mass_calc['processed_images'].tolist())

# Create a binary mapper
class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0}

# Apply class mapper to pathology column
mass_calc['labels'] = mass_calc['pathology'].replace(class_mapper)

# Check the number of classes
num_classes = len(mass_calc['labels'].unique())

num_classes

# Apply preprocessor to cropped data
# result_cropped = mass_calc['cropped_image_file_path'].apply(lambda x: pd.Series(image_processor(x, target_size), index=['cropped_image_array', 'cropped_skipped_count', 'cropped_processed_count']))

# Extract processed cropped images, skipped count, and processed count from the result
# mass_calc[['processed_cropped_images', 'cropped_skipped_count', 'cropped_processed_count']] = result_cropped

# Filter out rows where processed_cropped_images is None
# mass_calc = mass_calc.dropna(subset=['processed_cropped_images'])

# Convert the processed_cropped_images column to an array
# X_resized_cropped = np.array(mass_calc['processed_cropped_images'].tolist())

# Combine the processed images from both columns if needed
# X_combined = np.concatenate([X_resized, X_resized_cropped], axis=???)

# Create a binary mapper for the labels if not already created
# class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0}

# Apply class mapper to pathology column if not already applied
# mass_calc['labels'] = mass_calc['pathology'].replace(class_mapper)

# Check the number of classes
# num_classes = len(mass_calc['labels'].unique())

skipped_sum = mass_calc['skipped_count'].sum()
#+ mass_calc['cropped_skipped_count'].sum()
print(f"Total skipped count: {skipped_sum}")

processed_count = mass_calc['processed_count'].sum()
#+ mass_calc['cropped_processed_count'].sum()
print(f"Total processed count: {processed_count}")

mass_calc['processed_count'].sum()

# mass_calc['cropped_processed_count'].sum()

# Apply preprocessor to train data
# mass_calc[['processed_images', 'skipped_count', 'processed_count']] = mass_calc['image_file_path'].apply(
    # lambda x: pd.Series(image_processor(x, target_size), index=['image_array', 'skipped_count', 'processed_count']))

# Create a binary mapper
# class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0}

# Apply preprocessor to train data
# mass_calc['processed_images'] = mass_calc['image_file_path'].apply(lambda x: image_processor(x, target_size))

# Create a binary mapper
# class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0}

mass_calc

# Assuming your DataFrame is named 'your_dataframe'
# Replace 'your_generated_data.csv' with the desired file name
# mass_calc.to_csv('mass_calc.csv', index=False)

# from IPython.display import FileLink
# Generate a download link
# FileLink('mass_calc.csv')

# Convert the processed_images column to an array
# X_resized = np.array(mass_calc['processed_images'].tolist())

# Apply class mapper to pathology column
# mass_calc['labels'] = mass_calc['pathology'].replace(class_mapper)

# Check the number of classes
# num_classes = len(mass_calc['labels'].unique())

print('Number of Classes:', num_classes)

mass_calc.shape

"""##### V. Data Split for Train, Test and Validation"""

unique_labels_count = mass_calc['labels'].nunique()
print("Total number of unique labels:", unique_labels_count)

filled_labels_count = mass_calc['labels'].count()

print("Total number of filled labels:", filled_labels_count)

print(len(X_resized))
print(len(mass_calc['labels'].values))

mass_calc = mass_calc.dropna(subset=['labels'])

print(len(X_resized))
print(len(mass_calc['labels'].values))

# Assuming mass_calc is your DataFrame and 'labels' is the column of interest
mass_calc = mass_calc.dropna(subset=['labels'])

# Assuming X_resized is a NumPy array
# Remove corresponding rows from X_resized
X_resized = X_resized[mass_calc.index]

print(len(X_resized))
print(len(mass_calc['labels'].values))

# Split data into train, test, and validation sets (70, 20, 10)
X_train, X_temp, y_train, y_temp = train_test_split(X_resized, mass_calc['labels'].values, test_size=0.2, random_state=42)

# Split data into train, test, and validation sets (70, 20, 10)
# X_train, X_temp, y_train, y_temp = train_test_split(X_resized, mass_calc['labels'].values, test_size=0.2, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)

# Convert integer labels to one-hot encoded labels
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)
y_val = to_categorical(y_val, num_classes)

# Number of images in each set
num_train_images = X_train.shape[0]
num_test_images = X_test.shape[0]
num_val_images = X_val.shape[0]

print("Number of images in the training set:", num_train_images)
print("Number of images in the testing set:", num_test_images)
print("Number of images in the validation set:", num_val_images)

"""##### VI. Data Augmentation"""

# Create an ImageDataGenerator for each type of augmentation
generators = []
augmentation_configs = [
    dict(rotation_range=30),
    dict(horizontal_flip=True, vertical_flip=True),
    # Add more configurations as needed
]

# Create generators for each configuration
for config in augmentation_configs:
    generator = ImageDataGenerator(**config)
    generators.append(generator)

# Add the original data generator
generators.append(ImageDataGenerator())

# Combine the original data with augmented data
combined_generator = zip(*[gen.flow(X_train, y_train, batch_size=12) for gen in generators])

# Data_augmentation
train_datagen = ImageDataGenerator(rotation_range=30,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   vertical_flip=True,
                                   fill_mode='constant')

# Apply augmentation to training data
train_data_augmented = train_datagen.flow(X_train, y_train, batch_size=32)

# batch_size = 32
# total_images_before_augmentation = 2628

# Calculate the total number of batches generated
# total_batches = total_images_before_augmentation / batch_size

# Calculate the total number of images after augmentation
# total_images_after_augmentation = total_batches * batch_size

# print("Total images after augmentation:", total_images_after_augmentation)

"""##### VII. DenseNet 169"""

# Create the DenseNet 169 base model
base_model = DenseNet169(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))

# Freeze the layers of the base model
for layer in base_model.layers:
    layer.trainable = False

# Add custom classification layers on top of the base model
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
# x = Dropout(0.5)(x) Keeping the dropouts I didn't get good accuracy
predictions = Dense(2, activation='softmax')(x)
# Create the final model
model = Model(inputs=base_model.input, outputs=predictions)

model.summary()

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model using each batch from the combined generator
epochs = 5  # Adjust as needed
steps_per_epoch = len(X_train) // 12  # Adjust batch size as needed
steps_per_epoch

# Check the output shape of your model
print(model.output_shape)

# Define the EarlyStopping callback
early_stopping = EarlyStopping(
   monitor='val_accuracy',  # Monitor validation accuracy
   patience=5,  # Number of epochs with no improvement after which training will be stopped
   min_delta=0.00001,  # Minimum change in the monitored quantity to qualify as an improvement
   mode='max',  # 'max' if the quantity monitored should be increasing, 'min' if decreasing
   verbose=1  # Print a message when training is stopped due to early stopping
)

# Continue from where you left off in the previous code
for epoch in range(epochs):
    for step in range(steps_per_epoch):
        batch_data = next(combined_generator)
        # Unpack the batch_data
        X_batches, y_batches = zip(*batch_data)

        # Concatenate the batches along the batch axis
        X_batch = np.concatenate(X_batches, axis=0)
        y_batch = np.concatenate(y_batches, axis=0)

        # Assuming you have a model created and compiled
        # Replace `your_model` with the actual name of your model
        history = model.fit(X_batch, y_batch, validation_data=(X_val, y_val),steps_per_epoch=len(X_batch),validation_steps=len(X_val), epochs=1, callbacks=[early_stopping],batch_size=12, verbose=2)

    # Evaluate the model or perform other tasks at the end of each epoch
    # For example, you can print validation metrics or save the model weights
    # Replace `your_validation_data` and `your_validation_labels` with actual validation data
    # val_loss, val_accuracy = your_model.evaluate(your_validation_data, your_validation_labels)
    # print(f'Epoch {epoch + 1}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')

# After training, you may want to save the model or perform other tasks
# Replace `your_model.save` with the actual method for saving your model
# your_model.save('your_model.h5')

# Continue with the rest of the code
# history = model.fit(
#    train_data_augmented,
#    epochs=500,
#    validation_data=(X_val, y_val),
#    steps_per_epoch=len(X_train) // 32,
#    validation_steps=len(X_val),
#    callbacks=[early_stopping]  # Add the EarlyStopping callback
#)

# Calculate the total number of iterations
iterations = 4*5*len(X_train)//12 # Assuming 200 epochs and batch size of 32
print(f'Total number of iterations: {iterations}')

# Assuming train_data_augmented is a data generator
# history = model.fit(
#    train_data_augmented,
#    epochs=200,
#    validation_data=(X_val, y_val),
#    steps_per_epoch=len(train_data_augmented),
#    validation_steps=len(X_val),
#    callbacks=[early_stopping]  # Add the EarlyStopping callback
#)

# Calculate the total number of iterations
# iterations = 60 * len(train_data_augmented)
# print(f'Total number of iterations: {iterations}')

"""##### IV. Results"""

from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

def evaluate_model(model, X_data, y_data, dataset_type):
    y_pred = model.predict(X_data)

    # Convert predictions to binary (0 or 1)
    y_pred_binary = np.argmax(y_pred, axis=1)
    y_true_binary = np.argmax(y_data, axis=1)

    # Calculate metrics
    accuracy = accuracy_score(y_true_binary, y_pred_binary)
    recall = recall_score(y_true_binary, y_pred_binary)
    precision = precision_score(y_true_binary, y_pred_binary)
    f1 = f1_score(y_true_binary, y_pred_binary)
    confusion_mat = confusion_matrix(y_true_binary, y_pred_binary)
    print(f"{dataset_type} Accuracy: {accuracy:.4f}")
    print(f"{dataset_type} Recall: {recall:.4f}")
    print(f"{dataset_type} Precision: {precision:.4f}")
    print(f"{dataset_type} F1 Score: {f1:.4f}")
    print(f"{dataset_type} Confusion Matrix:\n{confusion_mat}\n")

    # Plot confusion matrix heatmap
    plt.figure(figsize=(8, 6))
    sns.heatmap(confusion_mat, annot=True, fmt="d", cmap="Blues", xticklabels=["BENIGN", "MALIGNANT"], yticklabels=["BENIGN", "MALIGNANT"])
    plt.title(f"{dataset_type} Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

evaluate_model(model, X_train, y_train, "Train")

evaluate_model(model, X_test, y_test, "Test")

evaluate_model(model, X_val, y_val, "Validation")

from sklearn.metrics import roc_curve, auc

# Use the trained model to predict probabilities for the test set
y_pred_prob = model.predict(X_test)

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test[:, 1], y_pred_prob[:, 1])
roc_auc = auc(fpr, tpr)

# Plot the ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.show()

# Print the AUC score
print(f'AUC: {roc_auc:.2f}')

history_dict = history.history

# plot training loss vs validation loss
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
acc = history_dict['accuracy']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, loss_values, 'b', label='Training Loss')
plt.plot(epochs, val_loss_values, 'r', label='Validation Loss')
plt.title('Training and Validation Loss', fontsize=12)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# plot training vs validation accuracy
val_acc_values = history_dict['val_accuracy']
acc = history_dict['accuracy']

plt.plot(epochs, acc, 'b', label='Training Accuracy')
plt.plot(epochs, val_acc_values, 'r', label='Validation Accuracy')
plt.title('Training and Validation Accuracy', fontsize=12)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()